# ğŸ Horse Locomotion: Cross-Modal Alignment via Self-Supervised Learning

This repository presents a partial implementation of a self-supervised cross-modal learning framework designed to align multi-sensor IMU time series with video representations for equine gait analysis.  
The system learns a robust sensor-only encoder by distilling knowledge from a frozen visual â€œteacherâ€ model.

---

## ğŸ“Œ Objectives

- Synchronize and align data from 5 IMU sensors with visual features extracted from VideoMAE-base  
- Learn a shared latent space using Sigmoid Contrastive Loss (SigLIP)  
- Enable accurate classification of horse gaits â€” Walk / Trot / Canter â€” using wearable data only  
- Provide a modular research codebase demonstrating:
  - Time-series Transformers  
  - Cross-modal representation learning  
  - Knowledge distillation from foundation models

---

## ğŸ›  Key Technical Features

- Sensor Fusion from 5 IMUs (Accelerometer + Gyroscope) creating a 30-channel input  
- PatchTST-like IMU Transformer Encoder with 25 patches Ã— 10 steps  
- Frozen VideoMAE-base (768-d) teacher backbone  
- SigLIP projection heads to 256-d shared space  
- Two-stage training: self-supervised pretraining + linear probe

---

## ğŸ“‚ Repository Structure

configs/        â†’ YAML configuration files  
src/datasets/   â†’ IMU/video loaders & synchronization logic  
src/models/     â†’ PatchTST encoder, VideoMAE wrapper, projections  
src/losses/     â†’ SigLIP contrastive objective  
src/train/      â†’ pretrain.py / probe.py  
src/eval/       â†’ metrics, confusion matrix, curves

| Directory | Description |
|---|---|
| configs | Hyperparameters for pretraining & probing |
| datasets | Cross-modal pair construction |
| models | Encoders and projection heads |
| losses | Alignment objectives |
| train | Training entry points |
| eval | Evaluation utilities |

---

## ğŸ”¬ Architecture in Detail

### 1. IMU Branch â€” Patch Transformer

- Input aggregation from:
  - Accelerometer (x, y, z)  
  - Gyroscope (x, y, z)  
- Total = 30 channels  
- 250 timesteps @ 50Hz

Processing pipeline:

IMU (30Ã—250)  
â†’ Patching (10,10) â†’ 25 patches  
â†’ CLS Token + Positional Encoding  
â†’ Transformer Encoder  
â†’ 256-d sensor embedding

---

### 2. Video Branch â€” Frozen Foundation Teacher

- Backbone: VideoMAE-base  
- Output: mean over tokens â†’ 768-d  
- Backbone remains frozen to prevent overfitting on limited datasets

---

### 3. Cross-Modal Alignment â€” SigLIP

- Both modalities projected to 256-d  
- Similarity matrix learned via sigmoid binary objectives  
- Diagonal â†’ positive pairs  
- Off-diagonal â†’ negatives

---

## ğŸ“ˆ Training Strategy

1. Stage A â€” Self-Supervised Pretraining  
   - Train IMU encoder + projections  
   - Align with frozen VideoMAE teacher

2. Stage B â€” Linear Probe  
   - Freeze IMU encoder  
   - Train lightweight classifier to validate embedding quality

